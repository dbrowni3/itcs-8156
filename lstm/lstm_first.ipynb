{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as ln\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTM(ln.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "\n",
    "        #the forget gate weights and bias \n",
    "        #(for f_t = sig(wf1 x_t + wf2 h_{t-1} + b_f))\n",
    "        self.wf1 = nn.Parameter(torch.normal(mean=mean,std=std),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        self.wf2 = nn.Parameter(torch.normal(mean=mean,std=std),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        self.bf = nn.Parameter(torch.tensor(0.0),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        #the input gate weights and bias\n",
    "        #(for i_t = sig(wi1 x_t + wi2 h_{t-1} + b_i))\n",
    "        self.wi1 = nn.Parameter(torch.normal(mean=mean,std=std),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        self.wi2 = nn.Parameter(torch.normal(mean=mean,std=std),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        self.bi = nn.Parameter(torch.tensor(0.0),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        #the output gate weights and bias\n",
    "        #(for o_t = sig(wo1 x_t + wo2 h_{t-1} + b_o))\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean=mean,std=std),\n",
    "                        requires_grad=True,\n",
    "                        )\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean=mean,std=std),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        self.bo = nn.Parameter(torch.tensor(0.0),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        #the candidate context weights and bias\n",
    "        #(for c^'_t = sig(wcc1 x_t + wcc2 h_{t-1} + bc_c))\n",
    "        self.wcc1 = nn.Parameter(torch.normal(mean=mean,std=std),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        self.wcc2 = nn.Parameter(torch.normal(mean=mean,std=std),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        self.bcc = nn.Parameter(torch.tensor(0.0),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "\n",
    "    def unit(self, val_in, long_mem, short_mem):\n",
    "        '''\n",
    "        INPUTS:\n",
    "            val_in - input into this step of the unit x_t\n",
    "\n",
    "            long_mem - the long term memory at this step\n",
    "\n",
    "            short_mem - the short term memory at this step\n",
    "        OUTPUTS:\n",
    "\n",
    "        \n",
    "        '''\n",
    "        f_t = torch.sigmoid((val_in*self.wf1)+(short_mem*self.wf2)+(self.bf))\n",
    "\n",
    "        i_t = torch.sigmoid((val_in*self.wi1)+(short_mem*self.wi2)+(self.bi))\n",
    "\n",
    "        cc_t = torch.tanh((val_in*self.wcc1)+(short_mem*self.wcc2)+(self.bcc))\n",
    "\n",
    "        #update the long term memory (c_t)\n",
    "        update_long_mem = (f_t*long_mem) + (i_t*cc_t)\n",
    "\n",
    "        o_t = torch.sigmoid((val_in*self.wo1)+(short_mem*self.wo2)+(self.bo))\n",
    "\n",
    "        #update the short term memory (h_t)\n",
    "        update_short_mem = o_t*torch.tanh(update_long_mem)\n",
    "\n",
    "        return ([update_long_mem, update_short_mem])\n",
    "\n",
    "\n",
    "    def forward(self,):\n",
    "        pass\n",
    "\n",
    "    def optimize(self):\n",
    "        pass\n",
    "\n",
    "    def train_step(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_pre_hooks', '_state_dict_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', 'exp_save_path', 'current_epoch', 'global_step', 'loaded_optimizer_states_dict', 'trainer', 'logger', 'use_dp', 'use_ddp', 'use_ddp2', 'use_tpu', 'use_amp', '_dtype', '_device', '_example_input_array'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BasicLSTM()\n",
    "\n",
    "model.__dict__.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theoryML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
