{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLSTM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_feat, num_hiddens, num_out):\n",
    "        '''\n",
    "        num_feat - number of features input into the model\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_feat = num_feat\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_out = num_out\n",
    "\n",
    "        shape_w1 = (num_hiddens,num_feat)\n",
    "        shape_w2 = (num_hiddens,num_hiddens)\n",
    "        \n",
    "        mean = 0.0\n",
    "        std = 1.0\n",
    "\n",
    "\n",
    "\n",
    "        #the forget gate weights and bias \n",
    "        #(for f_t = sig(wf1 x_t + wf2 h_{t-1} + b_f))\n",
    "        self.wf1 ,self.wf2, self.bf = self.initWeights(shape_w1, shape_w2, mean, std)\n",
    "\n",
    "        #the input gate weights and bias\n",
    "        #(for i_t = sig(wi1 x_t + wi2 h_{t-1} + b_i))\n",
    "        self.wi1, self.wi2, self.bi = self.initWeights(shape_w1, shape_w2, mean, std) \n",
    "        #the output gate weights and bias\n",
    "        #(for o_t = sig(wo1 x_t + wo2 h_{t-1} + b_o))\n",
    "        self.wo1, self.wo2, self.bo = self.initWeights(shape_w1, shape_w2, mean, std)\n",
    "        #the candidate context weights and bias\n",
    "        #(for c^'_t = sig(wcc1 x_t + wcc2 h_{t-1} + bc_c))\n",
    "        self.wcc1, self.wcc2, self.bcc = self.initWeights(shape_w1, shape_w2, mean, std)\n",
    "\n",
    "        self.whq = nn.Parameter(torch.normal(mean=mean,\n",
    "                                             std=std,\n",
    "                                             size=(num_hiddens, num_out)),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        self.bq = nn.Parameter(torch.tensor(0.0),\n",
    "                        requires_grad=True,\n",
    "                        )\n",
    "\n",
    "        print('wf1 \\n', self.wf1.shape, '\\n wf2 \\n', self.wf2.shape, '\\n bf \\n', self.bf.shape)\n",
    "        print('wi1 \\n', self.wi1.shape, '\\n wi2 \\n', self.wi2.shape, '\\n bi \\n', self.bi.shape)\n",
    "        print('wcc1 \\n', self.wf1.shape, '\\n wcc2 \\n', self.wf2.shape, '\\n bf \\n', self.bcc.shape)\n",
    "        print('wo1 \\n', self.wo1.shape, '\\n wo2 \\n', self.wo2.shape, '\\n bo \\n', self.bo.shape)\n",
    "\n",
    "    def initWeights(self, shape_w1, shape_w2, mean, std):\n",
    "        w1 = nn.Parameter(torch.normal(mean=mean,std=std, size=shape_w1),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        w2 = nn.Parameter(torch.normal(mean=mean,std=std,size=shape_w2),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        bias = nn.Parameter(torch.zeros(self.num_hiddens),\n",
    "                                requires_grad=True,\n",
    "                                )\n",
    "        return w1, w2, bias\n",
    "\n",
    "\n",
    "    def unit(self, val_in, long_mem, short_mem):\n",
    "        '''\n",
    "        INPUTS:\n",
    "            val_in - input into this step of the unit x_t\n",
    "\n",
    "            long_mem - the long term memory at this step\n",
    "\n",
    "            short_mem - the short term memory at this step\n",
    "        OUTPUTS:\n",
    "\n",
    "        \n",
    "        '''\n",
    "        val_in = val_in.float()\n",
    "        print(val_in)\n",
    "        print(self.wi1)\n",
    "        print()\n",
    "\n",
    "        i_t = torch.sigmoid((self.wi1@val_in)+(self.wi2@short_mem)+(self.bi))\n",
    "\n",
    "        f_t = torch.sigmoid((self.wf1@val_in)+(self.wf2@short_mem)+(self.bf))\n",
    "        \n",
    "        o_t = torch.sigmoid((self.wo1@val_in)+(self.wo2@short_mem)+(self.bo))\n",
    "\n",
    "        cc_t = torch.tanh((self.wcc1@val_in)+(short_mem@short_mem)+(self.bcc))\n",
    "\n",
    "        # print('f_t ', f_t)\n",
    "        # print('i_t ', i_t)\n",
    "        # print('cc_t ', cc_t)\n",
    "        # print('o_t ', o_t)\n",
    "\n",
    "        #update the long term memory (c_t)\n",
    "        c_t = (f_t*long_mem) + (i_t*cc_t)\n",
    "\n",
    "        #update the short term memory (h_t)\n",
    "        h_t = o_t*torch.tanh(c_t)\n",
    "        # print('update_short_mem.shape, ', update_short_mem.shape)\n",
    "\n",
    "        #for multiple inputs we not do a final layer that is just linear\n",
    "        # Y = (update_short_mem@self.whq) + self.bq\n",
    "        Y = h_t\n",
    "\n",
    "        return [Y, c_t, h_t]\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        in order case input should be an array with multiple inputs for the model.\n",
    "        The columns are the features and the rows are the days\n",
    "        '''\n",
    "        n_seq = np.shape(input)[-1]\n",
    "\n",
    "        # long_mem = torch.zeros(n_seq, requires_grad=False)\n",
    "        # short_mem = torch.zeros(n_seq, requires_grad=False)\n",
    "        long_mem = torch.zeros(self.num_hiddens)\n",
    "        short_mem = torch.zeros(self.num_hiddens)\n",
    "        \n",
    "        for ii in range(0,n_seq-1):\n",
    "\n",
    "            # long_mem[ii+1], short_mem[ii+1] = self.unit(input[ii], \n",
    "            #                                         long_mem[ii], \n",
    "            #                                         short_mem[ii],\n",
    "            #                                         )\n",
    "            # print(input[:,ii])\n",
    "            Y, long_mem, short_mem = self.unit(input[:,ii], \n",
    "                                                    long_mem, \n",
    "                                                    short_mem,\n",
    "                                                    )\n",
    "        return Y\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters())\n",
    "\n",
    "    def training_step(self, batch, batch_indx):\n",
    "\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "\n",
    "        loss = (output_i - label_i)**2\n",
    "\n",
    "        self.log(\"training loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "\n",
    "        test_loss = (output_i - label_i)**2\n",
    "\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "\n",
    "        return self(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# caution: path[0] is reserved for script path (or '' in REPL)\n",
    "sys.path.insert(1, r'D:\\Documents\\GitHub\\itcs-8156\\utils')\n",
    "\n",
    "from preprocessing import (market_prepro,\n",
    "                           lstm_timeseries_feat_and_targ,\n",
    "                           \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Volume', 'High-Low', 'Close-Open', 'Day_date',\n",
       "       'Month', 'Year', 'near_end_quarter', 'Day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1044, 11)\n",
      "(1044,)\n"
     ]
    }
   ],
   "source": [
    "# st = \"Stocks\"\n",
    "st = \"ETFs\"\n",
    "\n",
    "#Input stock name\n",
    "sn = \"aadr\" \n",
    "f = r'D:\\Desktop\\College Spring 2023\\machineLearning\\project\\coding\\data'\n",
    "X_train, X_test, T_train, T_test = market_prepro(f,st,sn,False,splitdata=True, stdzr='minmax')\n",
    "print(X_train.shape)\n",
    "print(T_train.shape)\n",
    "# X,T = market_prepro(f,st,sn,False,splitdata=False)\n",
    "\n",
    "dl_train, ds_train = lstm_timeseries_feat_and_targ(X_train[['Open','Low']], T_train, 4, 1,None)\n",
    "dl_test, ds_test = lstm_timeseries_feat_and_targ(X_test['Open'], T_test, 4, 1,  None)\n",
    "# dl_train, ds_train = lstm_timeseries_feat_and_targ(X_train['Open'], T_train, 3, 1, [ 'Year', 'Month' ,'Day_date', 'Day'])\n",
    "# dl_test, ds_test = lstm_timeseries_feat_and_targ(X_test['Open'], T_test, 3, 1, [ 'Year', 'Month' ,'Day_date', 'Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wf1 \n",
      " torch.Size([2, 2]) \n",
      " wf2 \n",
      " torch.Size([2, 2]) \n",
      " bf \n",
      " torch.Size([2])\n",
      "wi1 \n",
      " torch.Size([2, 2]) \n",
      " wi2 \n",
      " torch.Size([2, 2]) \n",
      " bi \n",
      " torch.Size([2])\n",
      "wcc1 \n",
      " torch.Size([2, 2]) \n",
      " wcc2 \n",
      " torch.Size([2, 2]) \n",
      " bf \n",
      " torch.Size([2])\n",
      "wo1 \n",
      " torch.Size([2, 2]) \n",
      " wo2 \n",
      " torch.Size([2, 2]) \n",
      " bo \n",
      " torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "mdl_stock = BasicLSTM(num_feat=2, num_hiddens=2, num_out=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0114, 0.0204, 0.0237, 0.0198],\n",
       "        [0.0023, 0.0146, 0.0132, 0.0172]], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0114, 0.0023])\n",
      "Parameter containing:\n",
      "tensor([[-1.6435, -0.0734],\n",
      "        [ 1.0014, -1.8881]], requires_grad=True)\n",
      "tensor([0.0204, 0.0146])\n",
      "Parameter containing:\n",
      "tensor([[-1.6435, -0.0734],\n",
      "        [ 1.0014, -1.8881]], requires_grad=True)\n",
      "tensor([0.0237, 0.0132])\n",
      "Parameter containing:\n",
      "tensor([[-1.6435, -0.0734],\n",
      "        [ 1.0014, -1.8881]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0015, -0.0130])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl_stock.forward(ds_train[0][0]).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\setup.py:176: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "------------------------------\n",
      "35        Trainable params\n",
      "0         Non-trainable params\n",
      "35        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "c:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1039 [00:00<?, ?it/s] "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`self.log(training loss, tensor([[0.0011, 0.0002]], dtype=torch.float64))` was called, but the tensor must have a single element. You can try doing `self.log(training loss, tensor([[0.0011, 0.0002]], dtype=torch.float64).mean())`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m mdl_stock \u001b[39m=\u001b[39m BasicLSTM(num_feat\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, num_hiddens\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, num_out\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m) \u001b[39m# with default learning rate, 0.001 (this tiny learning rate makes learning slow)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(mdl_stock, train_dataloaders\u001b[39m=\u001b[39;49mdl_train)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    606\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[0;32m    607\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 608\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    609\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    643\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    644\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[0;32m    645\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    646\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    648\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    649\u001b[0m )\n\u001b[1;32m--> 650\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    652\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1112\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1114\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1115\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1191\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(dataloader, batch_to_device\u001b[39m=\u001b[39mbatch_to_device)\n\u001b[0;32m    266\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 267\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:213\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    212\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 213\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(kwargs)\n\u001b[0;32m    215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    217\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m     85\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\n\u001b[0;32m     86\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mbatch_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m     87\u001b[0m     )\n\u001b[1;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(optimizers, kwargs)\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:202\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[1;34m(self, optimizers, kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, optimizers: List[Tuple[\u001b[39mint\u001b[39m, Optimizer]], kwargs: OrderedDict) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_kwargs(kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hiddens)\n\u001b[1;32m--> 202\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position])\n\u001b[0;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[0;32m    205\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[0;32m    206\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:249\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[1;34m(self, kwargs, optimizer)\u001b[0m\n\u001b[0;32m    241\u001b[0m         closure()\n\u001b[0;32m    243\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     \u001b[39m# the `batch_idx` is optional with inter-batch parallelism\u001b[39;00m\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[0;32m    251\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    253\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[0;32m    256\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:370\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    362\u001b[0m     rank_zero_deprecation(\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe NVIDIA/apex AMP implementation has been deprecated upstream. Consequently, its integration inside\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m PyTorch Lightning has been deprecated in v1.9.0 and will be removed in v2.0.0.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m return True.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    368\u001b[0m     )\n\u001b[0;32m    369\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39musing_native_amp\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprecision_plugin, MixedPrecisionPlugin)\n\u001b[1;32m--> 370\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    371\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    372\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    373\u001b[0m     batch_idx,\n\u001b[0;32m    374\u001b[0m     optimizer,\n\u001b[0;32m    375\u001b[0m     opt_idx,\n\u001b[0;32m    376\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    377\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[0;32m    378\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    379\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[0;32m    380\u001b[0m )\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    383\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1356\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1353\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m   1355\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1356\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1358\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1359\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1742\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_lbfgs)\u001b[0m\n\u001b[0;32m   1663\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1664\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1665\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1671\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1672\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1673\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1674\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1675\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1740\u001b[0m \n\u001b[0;32m   1741\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1742\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:169\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    168\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_idx, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:234\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[1;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(\n\u001b[0;32m    235\u001b[0m     optimizer, model\u001b[39m=\u001b[39;49mmodel, optimizer_idx\u001b[39m=\u001b[39;49mopt_idx, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[0;32m    236\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:119\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, optimizer, model, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    118\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[1;32m--> 119\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\torch\\optim\\adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 183\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    185\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    186\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:105\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m     93\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     94\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     98\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     99\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \n\u001b[0;32m    102\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:149\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    150\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:135\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 135\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[0;32m    137\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:419\u001b[0m, in \u001b[0;36mOptimizerLoop._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[39m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \n\u001b[0;32m    412\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m    A ``ClosureResult`` containing the training step output.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m training_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    420\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[0;32m    422\u001b[0m model_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mtraining_step_end\u001b[39m\u001b[39m\"\u001b[39m, training_step_output)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1491\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1494\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1496\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:378\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[0;32m    377\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[1;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[1;32mIn[2], line 136\u001b[0m, in \u001b[0;36mBasicLSTM.training_step\u001b[1;34m(self, batch, batch_indx)\u001b[0m\n\u001b[0;32m    132\u001b[0m output_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(input_i[\u001b[39m0\u001b[39m])\n\u001b[0;32m    134\u001b[0m loss \u001b[39m=\u001b[39m (output_i \u001b[39m-\u001b[39m label_i)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\n\u001b[1;32m--> 136\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog(\u001b[39m\"\u001b[39;49m\u001b[39mtraining loss\u001b[39;49m\u001b[39m\"\u001b[39;49m, loss)\n\u001b[0;32m    138\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\core\\module.py:444\u001b[0m, in \u001b[0;36mLightningModule.log\u001b[1;34m(self, name, value, prog_bar, logger, on_step, on_epoch, reduce_fx, enable_graph, sync_dist, sync_dist_group, add_dataloader_idx, batch_size, metric_attribute, rank_zero_only)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m/dataloader_idx_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m name:\n\u001b[0;32m    439\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    440\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou called `self.log` with the key `\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    441\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but it should not contain information about `dataloader_idx`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m     )\n\u001b[1;32m--> 444\u001b[0m value \u001b[39m=\u001b[39m apply_to_collection(value, (Tensor, numbers\u001b[39m.\u001b[39;49mNumber), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__to_tensor, name)\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mshould_reset_tensors(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_fx_name):\n\u001b[0;32m    447\u001b[0m     \u001b[39m# if we started a new epoch (running its first batch) the hook name has changed\u001b[39;00m\n\u001b[0;32m    448\u001b[0m     \u001b[39m# reset any tensors for the new hook name\u001b[39;00m\n\u001b[0;32m    449\u001b[0m     results\u001b[39m.\u001b[39mreset(metrics\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, fx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_fx_name)\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\lightning_utilities\\core\\apply_func.py:51\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     53\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[0;32m     55\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\patbo\\.conda\\envs\\theoryML\\lib\\site-packages\\pytorch_lightning\\core\\module.py:618\u001b[0m, in \u001b[0;36mLightningModule.__to_tensor\u001b[1;34m(self, value, name)\u001b[0m\n\u001b[0;32m    612\u001b[0m value \u001b[39m=\u001b[39m (\n\u001b[0;32m    613\u001b[0m     value\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    614\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor)\n\u001b[0;32m    615\u001b[0m     \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(value, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    616\u001b[0m )\n\u001b[0;32m    617\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mnumel(value) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 618\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    619\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`self.log(\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m)` was called, but the tensor must have a single element.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    620\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m You can try doing `self.log(\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m.mean())`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    621\u001b[0m     )\n\u001b[0;32m    622\u001b[0m value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m    623\u001b[0m \u001b[39mreturn\u001b[39;00m value\n",
      "\u001b[1;31mValueError\u001b[0m: `self.log(training loss, tensor([[0.0011, 0.0002]], dtype=torch.float64))` was called, but the tensor must have a single element. You can try doing `self.log(training loss, tensor([[0.0011, 0.0002]], dtype=torch.float64).mean())`"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(max_epochs=10) # with default learning rate, 0.001 (this tiny learning rate makes learning slow)\n",
    "trainer.fit(mdl_stock, train_dataloaders=dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(mdl_stock, dataloaders=dl_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makepred(model, dataset):\n",
    "    y = []\n",
    "    t = []\n",
    "    for ii in dataset:\n",
    "        feat, lab = ii\n",
    "\n",
    "        y.append(model.forward(feat).detach().numpy()[0])\n",
    "        t.append(lab.numpy()[0])\n",
    "\n",
    "    return y, t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test, t_test = makepred(mdl_stock, ds_test)\n",
    "y_train, t_train = makepred(mdl_stock, ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[0:10])\n",
    "print(t_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (mean_absolute_percentage_error,\n",
    "                             r2_score,\n",
    ")\n",
    "\n",
    "# train_mape = mean_absolute_percentage_error(t_train, y_train)\n",
    "# test_mape = mean_absolute_percentage_error(t_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# print('Training MAPE ', train_mape)\n",
    "# print('Testing MAPE ', test_mape)\n",
    "\n",
    "print('Training r2 ', r2_score(t_train, y_train) )\n",
    "print('Testing r2 ', r2_score(t_test, y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scatter_results(Y,T,title):\n",
    "    plt.figure(figsize=(9,9))\n",
    "    plt.scatter(Y,T)\n",
    "    plt.xlabel('Model Prediction')\n",
    "    plt.ylabel('True Value')\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_results(y_test, t_test, 'Testing')\n",
    "scatter_results(y_train, t_train, 'Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(t_test)),t_test)\n",
    "plt.scatter(range(len(y_test)),y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "theoryML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
